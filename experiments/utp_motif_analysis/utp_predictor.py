#!/usr/bin/env python3
"""
uTP Variant Predictor from Mature Domain Embeddings

This script trains classifiers to predict uTP variant from mature protein
domain sequences using ProtT5 embeddings.

Key finding: The mature domain can predict uTP variant with ~68% accuracy
(p=0.001), suggesting co-evolution or a bipartite targeting system.

Outputs:
- mature-embeddings.h5: ProtT5 embeddings for all mature domains
- best-embedding-classifier.joblib: Best trained classifier
- classifier_results.csv: Performance comparison
- roc_curve.svg: ROC curve for best classifier
- confusion_matrix.svg: Confusion matrix

Usage:
    uv run python experiments/utp_motif_analysis/utp_predictor.py

Note: First run will download ProtT5 model (~3GB) and compute embeddings.
"""

import gc
import re
import warnings
from collections import Counter
from pathlib import Path

import h5py
import joblib
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from Bio import SeqIO
from scipy.stats import binomtest
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    f1_score,
    roc_curve,
    auc,
)
from sklearn.model_selection import permutation_test_score, train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from tqdm import tqdm

# Configuration
C_TERM_START = 880  # Start of uTP region (mature domain ends here)
MIN_OCCURRENCES = 10  # Minimum samples per class for training
TEST_SIZE = 0.2
N_PERMUTATIONS = 1000
RANDOM_STATE = 42

# Paths
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "data"
OUTPUT_DIR = SCRIPT_DIR / "output"

# Input files
MEME_FILE = DATA_DIR / "meme_gb.xml"
FULL_SEQS_FILE = OUTPUT_DIR / "good-c-term-full.fasta"  # Generated by script 1

# Output files
EMBEDDINGS_FILE = OUTPUT_DIR / "mature-embeddings.h5"
CLASSIFIER_FILE = OUTPUT_DIR / "best-embedding-classifier.joblib"

# ProtT5 model
PROTT5_MODEL = "Rostlab/prot_t5_xl_uniref50"

# Styling
sns.set_theme("paper")
matplotlib.rcParams.update({
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'font.size': 12,
    'legend.fontsize': 10,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10
})


def ensure_output_dir():
    """Create output directory if it doesn't exist."""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_sequences_and_motifs(meme_file: Path, seqs_file: Path) -> tuple[dict, dict]:
    """
    Load sequences and their motif assignments.
    
    Returns:
        Tuple of (sequences_dict, motifs_dict) where:
        - sequences_dict: {seq_name: sequence_string}
        - motifs_dict: {seq_name: tuple_of_motif_ids}
    """
    import xml.etree.ElementTree as ET
    from collections import defaultdict
    
    # Parse MEME XML
    meme_xml = ET.parse(meme_file)
    
    # Get sequence name mapping
    seq_names = {
        tag.attrib["id"]: tag.attrib["name"]
        for tag in meme_xml.findall(".//sequence")
    }
    
    # Extract motif assignments per sequence
    sites = meme_xml.findall(".//scanned_sites")
    sequences_motifs = defaultdict(list)
    for tag in sites:
        seq_id = tag.attrib["sequence_id"]
        for site in tag.findall("scanned_site"):
            sequences_motifs[seq_id].append(site.attrib)
    
    # Sort motifs by position and convert to tuple of IDs
    motifs_dict = {}
    for seq_id, motifs in sequences_motifs.items():
        if seq_id not in seq_names:
            continue
        seq_name = seq_names[seq_id]
        sorted_motifs = sorted(motifs, key=lambda m: int(m["position"]))
        motifs_dict[seq_name] = tuple(m["motif_id"] for m in sorted_motifs)
    
    # Load sequences
    seqs = {s.id: str(s.seq) for s in SeqIO.parse(seqs_file, "fasta")}
    
    # Filter to sequences that have both sequence and motif data
    common_names = set(seqs.keys()) & set(motifs_dict.keys())
    seqs = {k: v for k, v in seqs.items() if k in common_names}
    motifs_dict = {k: v for k, v in motifs_dict.items() if k in common_names}
    
    print(f"Loaded {len(seqs)} sequences with motif assignments")
    
    return seqs, motifs_dict


def extract_mature_domains(seqs: dict[str, str]) -> dict[str, str]:
    """Extract mature domain (everything before C-terminal uTP region)."""
    return {
        name: seq[:C_TERM_START].replace("-", "").replace("*", "")
        for name, seq in seqs.items()
    }


def load_prott5_model(device: str = "cpu"):
    """Load ProtT5 tokenizer and model."""
    from transformers import T5EncoderModel, T5Tokenizer
    
    print(f"Loading ProtT5 model (this may take a while on first run)...")
    tokenizer = T5Tokenizer.from_pretrained(PROTT5_MODEL, do_lower_case=False)
    model = T5EncoderModel.from_pretrained(PROTT5_MODEL)
    model = model.to(device)
    model.eval()
    
    gc.collect()
    if device == "cuda":
        torch.cuda.empty_cache()
    
    return tokenizer, model


def embed_sequences(
    sequences: list[str],
    tokenizer,
    model,
    device: str = "cpu",
    batch_size: int = 1
) -> list[np.ndarray]:
    """
    Generate ProtT5 embeddings for a list of sequences.
    
    Args:
        sequences: List of amino acid sequences
        tokenizer: ProtT5 tokenizer
        model: ProtT5 model
        device: 'cpu' or 'cuda'
        batch_size: Batch size for inference
    
    Returns:
        List of numpy arrays, each of shape (seq_len, 1024)
    """
    embeddings = []
    
    for seq in tqdm(sequences, desc="Computing embeddings"):
        # Clean sequence
        seq = re.sub(r"[UZOJB*]", "X", seq)
        seq_spaced = " ".join(seq)
        
        # Tokenize
        ids = tokenizer.batch_encode_plus(
            [seq_spaced],
            add_special_tokens=True,
            padding=True
        )
        input_ids = torch.tensor(ids["input_ids"]).to(device)
        attention_mask = torch.tensor(ids["attention_mask"]).to(device)
        
        # Get embeddings
        with torch.no_grad():
            output = model(input_ids=input_ids, attention_mask=attention_mask)
        
        embedding = output.last_hidden_state.cpu().numpy()
        
        # Remove padding and special tokens
        seq_len = (attention_mask[0] == 1).sum().item()
        seq_embedding = embedding[0][:seq_len - 1]  # -1 for </s> token
        
        embeddings.append(seq_embedding.astype(np.float32))
    
    return embeddings


def load_or_compute_embeddings(
    mature_domains: dict[str, str],
    embeddings_file: Path,
    device: str = "cpu"
) -> dict[str, np.ndarray]:
    """
    Load embeddings from file or compute them if not available.
    
    Returns:
        Dictionary mapping sequence name to embedding array
    """
    if embeddings_file.exists():
        print(f"Loading existing embeddings from {embeddings_file}")
        embeddings = {}
        with h5py.File(embeddings_file, "r") as f:
            for key in f.keys():
                # Key format: "idx_seqname"
                parts = key.split("_", 1)
                if len(parts) == 2:
                    seq_name = parts[1]
                    embeddings[seq_name] = f[key][()]
        
        # Check if all sequences have embeddings
        missing = set(mature_domains.keys()) - set(embeddings.keys())
        if missing:
            print(f"Warning: {len(missing)} sequences missing embeddings, recomputing all")
        else:
            return embeddings
    
    # Compute embeddings
    print("Computing ProtT5 embeddings (this will take a while)...")
    tokenizer, model = load_prott5_model(device)
    
    seq_names = list(mature_domains.keys())
    seq_list = [mature_domains[n] for n in seq_names]
    
    embedding_list = embed_sequences(seq_list, tokenizer, model, device)
    
    # Save to HDF5
    embeddings = {}
    with h5py.File(embeddings_file, "w") as f:
        for idx, (name, emb) in enumerate(zip(seq_names, embedding_list)):
            f.create_dataset(f"{idx}_{name}", data=emb)
            embeddings[name] = emb
    
    print(f"Saved embeddings to {embeddings_file}")
    
    # Clean up
    del tokenizer, model
    gc.collect()
    
    return embeddings


def prepare_dataset(
    embeddings: dict[str, np.ndarray],
    motifs_dict: dict[str, tuple],
    min_occurrences: int = MIN_OCCURRENCES
) -> tuple[np.ndarray, np.ndarray, list[tuple], list[str]]:
    """
    Prepare dataset for classification.
    
    Returns:
        Tuple of (X, Y, motif_labels, seq_names) where:
        - X: Feature matrix (mean-pooled embeddings)
        - Y: Label array (motif combination indices)
        - motif_labels: List of unique motif combinations
        - seq_names: List of sequence names in same order as X
    """
    # Get common sequences
    common = set(embeddings.keys()) & set(motifs_dict.keys())
    
    # Create feature matrix (mean-pool embeddings)
    X = []
    Y_raw = []
    seq_names = []
    
    for name in common:
        if motifs_dict[name]:  # Skip empty motif assignments
            X.append(np.mean(embeddings[name], axis=0))
            Y_raw.append(motifs_dict[name])
            seq_names.append(name)
    
    X = np.array(X)
    
    # Create label encoding
    motif_labels = list(set(Y_raw))
    Y = np.array([motif_labels.index(m) for m in Y_raw])
    
    # Filter to classes with minimum occurrences
    label_counts = Counter(Y)
    valid_labels = [l for l, c in label_counts.items() if c >= min_occurrences]
    
    mask = np.isin(Y, valid_labels)
    X = X[mask]
    Y = Y[mask]
    seq_names = [n for n, m in zip(seq_names, mask) if m]
    
    # Re-encode labels after filtering
    unique_labels = sorted(set(Y))
    label_map = {old: new for new, old in enumerate(unique_labels)}
    Y = np.array([label_map[y] for y in Y])
    motif_labels = [motif_labels[l] for l in unique_labels]
    
    print(f"Dataset: {len(X)} samples, {len(motif_labels)} classes")
    for i, label in enumerate(motif_labels):
        count = np.sum(Y == i)
        pattern = " â†’ ".join(m.replace("motif_", "") for m in label)
        print(f"  Class {i} ({pattern}): {count} samples")
    
    return X, Y, motif_labels, seq_names


def train_and_evaluate_classifiers(
    X: np.ndarray,
    Y: np.ndarray,
    n_permutations: int = N_PERMUTATIONS
) -> dict:
    """
    Train multiple classifiers and evaluate with permutation tests.
    
    Returns:
        Dictionary of results for each classifier
    """
    X_train, X_test, Y_train, Y_test = train_test_split(
        X, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=Y
    )
    
    classifiers = {
        "Logistic Regression": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),
        "Decision Tree": DecisionTreeClassifier(random_state=RANDOM_STATE),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),
        "SVC": SVC(probability=True, random_state=RANDOM_STATE)
    }
    
    results = {}
    best_accuracy = 0
    best_classifier = None
    best_name = None
    
    for name, clf in classifiers.items():
        print(f"\nTraining {name}...")
        
        # Fit classifier
        clf.fit(X_train, Y_train)
        
        # Evaluate on test set
        Y_pred = clf.predict(X_test)
        accuracy = accuracy_score(Y_test, Y_pred)
        f1 = f1_score(Y_test, Y_pred, average='weighted')
        
        # Binomial test (vs random guessing)
        n = len(Y_test)
        p = 1 / len(np.unique(Y))
        binom_pval = binomtest(round(accuracy * n), n, p, alternative='greater').pvalue
        
        # Permutation test
        print(f"  Running permutation test ({n_permutations} permutations)...")
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            score, perm_scores, perm_pval = permutation_test_score(
                clf, X, Y,
                scoring="accuracy",
                cv=5,
                n_permutations=n_permutations,
                n_jobs=-1,
                random_state=RANDOM_STATE
            )
        
        results[name] = {
            "classifier": clf,
            "accuracy": accuracy,
            "f1_score": f1,
            "cv_score": score,
            "permutation_scores": perm_scores,
            "permutation_pvalue": perm_pval,
            "binomial_pvalue": binom_pval,
            "Y_test": Y_test,
            "Y_pred": Y_pred
        }
        
        print(f"  Accuracy: {accuracy:.2%}")
        print(f"  F1 Score: {f1:.2f}")
        print(f"  Permutation p-value: {perm_pval:.4f}")
        
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_classifier = clf
            best_name = name
    
    results["_best"] = {
        "name": best_name,
        "classifier": best_classifier
    }
    
    return results


def plot_roc_curve(
    results: dict,
    X: np.ndarray,
    Y: np.ndarray,
    output_file: Path
):
    """Plot ROC curve for the best classifier."""
    best_name = results["_best"]["name"]
    clf = results["_best"]["classifier"]
    
    X_train, X_test, Y_train, Y_test = train_test_split(
        X, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=Y
    )
    
    # Get probability predictions
    if hasattr(clf, "predict_proba"):
        Y_score = clf.predict_proba(X_test)
    else:
        print("Classifier doesn't support predict_proba, skipping ROC curve")
        return
    
    # Binarize labels for multi-class ROC
    n_classes = len(np.unique(Y))
    Y_bin = label_binarize(Y_test, classes=np.arange(n_classes))
    
    # Compute ROC curve for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(Y_bin[:, i], Y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Plot
    plt.figure(figsize=(10, 8))
    colors = plt.cm.tab10(np.linspace(0, 1, n_classes))
    
    for i in range(n_classes):
        plt.plot(
            fpr[i], tpr[i],
            color=colors[i],
            lw=2,
            label=f'Class {i} (AUC = {roc_auc[i]:.2f})'
        )
    
    plt.plot([0, 1], [0, 1], 'k--', lw=1)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {best_name}')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"Saved ROC curve to {output_file}")


def plot_confusion_matrix(results: dict, motif_labels: list, output_file: Path):
    """Plot confusion matrix for the best classifier."""
    best_name = results["_best"]["name"]
    Y_test = results[best_name]["Y_test"]
    Y_pred = results[best_name]["Y_pred"]
    
    cm = confusion_matrix(Y_test, Y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=[f"uTP{i+1}" for i in range(len(motif_labels))],
        yticklabels=[f"uTP{i+1}" for i in range(len(motif_labels))]
    )
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix - {best_name}')
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"Saved confusion matrix to {output_file}")


def save_results(results: dict, motif_labels: list, output_dir: Path):
    """Save classification results to CSV."""
    rows = []
    for name, res in results.items():
        if name.startswith("_"):
            continue
        rows.append({
            "Classifier": name,
            "Accuracy": f"{res['accuracy']:.0%}",
            "F1 Score": f"{res['f1_score']:.2f}",
            "CV Score": f"{res['cv_score']:.2%}",
            "Permutation p-value": f"{res['permutation_pvalue']:.4f}",
            "Binomial p-value": f"{res['binomial_pvalue']:.4e}"
        })
    
    df = pd.DataFrame(rows)
    df = df.sort_values("Accuracy", ascending=False)
    df.to_csv(output_dir / "classifier_results.csv", index=False)
    
    print(f"\nClassifier Results:")
    print(df.to_string(index=False))
    print(f"\nSaved to classifier_results.csv")


def main():
    """Main analysis pipeline."""
    print("=" * 60)
    print("uTP Variant Predictor from Mature Domain Embeddings")
    print("=" * 60)
    
    ensure_output_dir()
    
    # Check input files
    if not MEME_FILE.exists():
        raise FileNotFoundError(
            f"MEME file not found: {MEME_FILE}\n"
            f"Please copy meme_gb.xml to {DATA_DIR}/"
        )
    if not FULL_SEQS_FILE.exists():
        raise FileNotFoundError(
            f"Sequences file not found: {FULL_SEQS_FILE}\n"
            f"Please run motif_combination_extraction.py first, or copy good-c-term-full.fasta to {OUTPUT_DIR}/"
        )
    
    # Determine device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Step 1: Load data
    print("\n[Step 1] Loading sequences and motif assignments...")
    seqs, motifs_dict = load_sequences_and_motifs(MEME_FILE, FULL_SEQS_FILE)
    
    # Step 2: Extract mature domains
    print("\n[Step 2] Extracting mature domains...")
    mature_domains = extract_mature_domains(seqs)
    
    # Step 3: Load or compute embeddings
    print("\n[Step 3] Loading/computing ProtT5 embeddings...")
    embeddings = load_or_compute_embeddings(mature_domains, EMBEDDINGS_FILE, device)
    
    # Step 4: Prepare dataset
    print("\n[Step 4] Preparing classification dataset...")
    X, Y, motif_labels, seq_names = prepare_dataset(embeddings, motifs_dict)
    
    if len(np.unique(Y)) < 2:
        print("ERROR: Not enough classes for classification")
        return
    
    # Step 5: Train and evaluate classifiers
    print("\n[Step 5] Training classifiers...")
    results = train_and_evaluate_classifiers(X, Y, n_permutations=N_PERMUTATIONS)
    
    # Step 6: Save best classifier
    print("\n[Step 6] Saving best classifier...")
    best_clf = results["_best"]["classifier"]
    joblib.dump(best_clf, CLASSIFIER_FILE)
    print(f"Saved to {CLASSIFIER_FILE}")
    
    # Also save motif labels for prediction
    joblib.dump(motif_labels, OUTPUT_DIR / "motif_labels.joblib")
    
    # Step 7: Generate plots
    print("\n[Step 7] Generating plots...")
    plot_roc_curve(results, X, Y, OUTPUT_DIR / "roc_curve.svg")
    plot_confusion_matrix(results, motif_labels, OUTPUT_DIR / "confusion_matrix.svg")
    
    # Step 8: Save results
    print("\n[Step 8] Saving results...")
    save_results(results, motif_labels, OUTPUT_DIR)
    
    print("\n" + "=" * 60)
    print("Analysis complete!")
    print(f"Results saved to: {OUTPUT_DIR}")
    print("=" * 60)
    
    # Summary
    best_name = results["_best"]["name"]
    best_acc = results[best_name]["accuracy"]
    best_pval = results[best_name]["permutation_pvalue"]
    
    print(f"\nKey Finding:")
    print(f"  Best classifier: {best_name}")
    print(f"  Accuracy: {best_acc:.0%}")
    print(f"  Permutation p-value: {best_pval:.4f}")
    print(f"\nThe mature domain can predict uTP variant significantly better than chance,")
    print(f"supporting the hypothesis of co-evolution or a bipartite targeting system.")


if __name__ == "__main__":
    main()
